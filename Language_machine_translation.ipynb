{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDYzT7IioFnN",
        "outputId": "4b47654a-e76d-4e1a-c39b-cb699144d555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total english sentences: 6414\n",
            "Total urdu sentences: 6414\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Quran English and Urdu translations\n",
        "with open('/content/Quran-EN (1)', 'r', encoding='utf-8') as f:\n",
        "    eng_lines = f.read().strip().split('\\n')\n",
        "\n",
        "with open('/content/Quran-UR (1)', 'r', encoding='utf-8') as f:\n",
        "    ur_lines = f.read().strip().split('\\n')\n",
        "\n",
        "# Add <start> and <end> tokens to English sentences\n",
        "en_lines = ['<start> ' + line + ' <end>' for line in eng_lines]\n",
        "\n",
        "print(f\"Total english sentences: {len(eng_lines)}\")\n",
        "print(f\"Total urdu sentences: {len(ur_lines)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ur_lines = ['<start> ' + line + ' <end>' for line in ur_lines]"
      ],
      "metadata": {
        "id": "nNk4kd3eurrb"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ur_lines[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho2lFFWrwcIw",
        "outputId": "ae336234-68db-4032-804e-3f75783f1890"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> \\ufeffسب تعریفیں اللہ ہی کے لئے ہیں جو تمام جہانوں کی پرورش فرمانے والا ہے ۔ <end>',\n",
              " '<start> نہایت مہربان بہت رحم فرمانے والا ہے ۔ <end>',\n",
              " '<start> روزِ جزا کا مالک ہے ۔ <end>',\n",
              " '<start> اے اللہ ! ہم تیری ہی عبادت کرتے ہیں اور ہم تجھ ہی سے مدد چاہتے ہیں ۔ <end>',\n",
              " '<start> ہمیں سیدھا راستہ دکھا ۔ <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Urdu tokenizer (input)\n",
        "ur_tokenizer = Tokenizer(filters='')\n",
        "ur_tokenizer.fit_on_texts(ur_lines)\n",
        "ur_tensor = ur_tokenizer.texts_to_sequences(ur_lines)\n",
        "ur_tensor = pad_sequences(ur_tensor, padding='post')\n",
        "\n",
        "# English tokenizer (target)\n",
        "en_tokenizer = Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(en_lines)  # ✅ Corrected\n",
        "en_tensor = en_tokenizer.texts_to_sequences(en_lines)  # ✅ Corrected\n",
        "en_tensor = pad_sequences(en_tensor, padding='post')"
      ],
      "metadata": {
        "id": "8mBgE6VRw122"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vflgY1fw7ksq",
        "outputId": "782363a6-b456-4d8d-a4cf-0d7e7e791562"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "    ur_tensor, en_tensor, test_size=0.1)\n",
        "\n",
        "print(\"Train size:\", len(input_tensor_train))\n",
        "print(\"Validation size:\", len(input_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4vD-qyXyPWc",
        "outputId": "89100dd8-1d33-4e25-c753-f911b3eac659"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 5772\n",
            "Validation size: 642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ur_vocab_size = len(ur_tokenizer.word_index) + 1\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Urdu vocab size:\", ur_vocab_size)\n",
        "print(\"English vocab size:\", en_vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ7pfDagyQsF",
        "outputId": "3e565d20-d1df-4f4f-e787-7136d39c02a7"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urdu vocab size: 8144\n",
            "English vocab size: 8138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "import tensorflow as tf\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n"
      ],
      "metadata": {
        "id": "mdTsGbmG0bNM"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)  # for encoder hidden states\n",
        "        self.W2 = tf.keras.layers.Dense(units)  # for decoder hidden state\n",
        "        self.V = tf.keras.layers.Dense(1)       # for calculating the attention score\n",
        "\n",
        "    def call(self, encoder_outputs, decoder_hidden):\n",
        "        # encoder_outputs shape: all hidden states\n",
        "        # decoder_hidden shape: current hidden state\n",
        "        # Wused for dimension same\n",
        "        decoder_hidden_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "        # Score calculation (alignment model)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_time_axis)\n",
        "        ))  # score shape: (batch_size, max_length, 1)\n",
        "\n",
        "        # Attention weights (softmax over time steps) gets distribution of all tokens\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Context vector: weighted sum of encoder_outputs\n",
        "        context_vector = attention_weights * encoder_outputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "-n6HsGfo9Rd9"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, decoder_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(decoder_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "        self.attention = BahdanauAttention(decoder_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "\n",
        "        # Step 1: Attention\n",
        "        context_vector, attention_weights = self.attention(enc_output, hidden)\n",
        "\n",
        "        # Step 2: Embed current input word (like <start>, then next predicted words)\n",
        "        x = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Step 3: Concatenate context vector with embedded input word\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # Step 4: Pass through LSTM\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "\n",
        "        # Step 5: Output shape -> (batch_size, vocab_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state_h, state_c, attention_weights\n"
      ],
      "metadata": {
        "id": "KmNdEhoyEGux"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(inp_sentence):\n",
        "    # Preprocess input sentence\n",
        "    inp_sentence = '<start> ' + inp_sentence + ' <end>'  # Add tokens\n",
        "    inp_sentence_seq = ur_tokenizer.texts_to_sequences([inp_sentence])\n",
        "    inp_sentence_seq = pad_sequences(inp_sentence_seq, padding='post', maxlen=input_tensor_train.shape[1])\n",
        "\n",
        "    # Get the encoder's output\n",
        "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp_sentence_seq)\n",
        "\n",
        "    # Start the translation (decoder input: <start> token)\n",
        "    start_token = en_tokenizer.word_index['<start>']\n",
        "    dec_input = tf.expand_dims([start_token], 1)\n",
        "\n",
        "    # Initialize with encoder's hidden state\n",
        "    dec_hidden_h = enc_hidden_h\n",
        "    dec_hidden_c = enc_hidden_c\n",
        "\n",
        "    predicted_sentence = []\n",
        "\n",
        "    # Run the decoder to predict words step by step\n",
        "    for t in range(1, en_tensor.shape[1]):  # Max sentence length\n",
        "        predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden_h, enc_output)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=1).numpy()[0]  # Get the predicted word index\n",
        "\n",
        "        if predicted_id == 0:  # Skip padding\n",
        "            continue\n",
        "\n",
        "        predicted_word = en_tokenizer.index_word.get(predicted_id, '<unknown>')\n",
        "\n",
        "        if predicted_word == '<end>':\n",
        "            break\n",
        "\n",
        "        if predicted_word != '<start>':  # Don't add <start> to the result\n",
        "            predicted_sentence.append(predicted_word)\n",
        "\n",
        "        # Use the predicted word as the next input to the decoder\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return ' '.join(predicted_sentence)"
      ],
      "metadata": {
        "id": "DlsrsJbz22M2"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score():\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    # Only evaluate a sample for faster computation\n",
        "    sample_size = min(100, len(input_tensor_val))\n",
        "\n",
        "    for i in range(sample_size):\n",
        "        # Get input sentence\n",
        "        inp_sentence = ur_tokenizer.sequences_to_texts([input_tensor_val[i]])[0]\n",
        "\n",
        "        # Get target sentence and clean it\n",
        "        targ_sentence = en_tokenizer.sequences_to_texts([target_tensor_val[i]])[0]\n",
        "        targ_sentence = targ_sentence.replace('<start>', '').replace('<end>', '').strip()\n",
        "\n",
        "        # Translate the input sentence\n",
        "        predicted_sentence = translate(inp_sentence)\n",
        "\n",
        "        # Add to lists\n",
        "        references.append([targ_sentence.split()])\n",
        "        hypotheses.append(predicted_sentence.split())\n",
        "\n",
        "    # Calculate BLEU score with different n-grams\n",
        "    bleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0))\n",
        "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2: {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3: {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4: {bleu4:.4f}\")\n",
        "\n",
        "    return bleu4"
      ],
      "metadata": {
        "id": "9rG40YK627kh"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # Mask padded tokens\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "BWm482SiETI8"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ensure eager execution\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "tJDYPtDU228C"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden_h, enc_hidden_c):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
        "\n",
        "        dec_hidden_h = enc_hidden_h\n",
        "        dec_hidden_c = enc_hidden_c\n",
        "\n",
        "        dec_input = tf.expand_dims([en_tokenizer.word_index['<start>']] * inp.shape[0], 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, dec_hidden_h, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n"
      ],
      "metadata": {
        "id": "BBEbVvtvbe4C"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "encoder = Encoder(ur_vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(en_vocab_size, embedding_dim, units)\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        enc_hidden_h = tf.zeros((BATCH_SIZE, units))\n",
        "        enc_hidden_c = tf.zeros((BATCH_SIZE, units))\n",
        "\n",
        "        batch_loss = train_step(inp, targ, enc_hidden_h, enc_hidden_c)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP8Tf7g8btUl",
        "outputId": "3f647c57-fb9c-4933-9a8f-7f02a7edfd8a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 0.9368\n",
            "Epoch 2 Loss 0.8173\n",
            "Epoch 3 Loss 0.7641\n",
            "Epoch 4 Loss 0.7199\n",
            "Epoch 5 Loss 0.6828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BLEU score on validation set\n",
        "print(\"Calculating BLEU score...\")\n",
        "bleu_score = calculate_bleu_score()\n",
        "print(f\"Final BLEU-4 Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Test the translation with a few examples\n",
        "test_sentences = [\n",
        "    ur_lines[0].replace('<start>', '').replace('<end>', '').strip(),\n",
        "    ur_lines[5].replace('<start>', '').replace('<end>', '').strip(),\n",
        "    ur_lines[10].replace('<start>', '').replace('<end>', '').strip()\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate(sentence)\n",
        "    print(f\"Urdu: {sentence}\")\n",
        "    print(f\"Translation: {translation}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx4V5DNUccPl",
        "outputId": "e39143b7-c929-429e-a992-0c84190917d0"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BLEU score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.0397\n",
            "BLEU-2: 0.0141\n",
            "BLEU-3: 0.0044\n",
            "BLEU-4: 0.0000\n",
            "Final BLEU-4 Score: 0.0000\n",
            "Urdu: ﻿سب تعریفیں اللہ ہی کے لئے ہیں جو تمام جہانوں کی پرورش فرمانے والا ہے ۔\n",
            "Translation: ( o beloved ! ) we have been sent down to the people of the earth .\n",
            "--------------------------------------------------\n",
            "Urdu: ان لوگوں کا راستہ جن پر تو نے انعام فرمایا ۔\n",
            "Translation: ( o beloved ! )\n",
            "--------------------------------------------------\n",
            "Urdu: اور وہ لوگ جو آپ کی طرف نازل کیا گیا اور جو آپ سے پہلے نازل کیا گیا سب پر ایمان لاتے ہیں ، اور وہ آخرت پر بھی کامل یقین رکھتے ہیں ۔\n",
            "Translation: and ( o beloved ! ) we have been sent down to the earth .\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8lWut40h00Gy"
      },
      "execution_count": 123,
      "outputs": []
    }
  ]
}